import fs from 'node:fs'
import path from 'node:path'
import { sValidator } from '@hono/standard-validator'
import { Hono } from 'hono'
import { env } from 'hono/adapter'
import { streamSSE } from 'hono/streaming'
import { validator } from 'hono/validator'
import OpenAI from 'openai'
import type { Stream } from 'openai/streaming'
import { renderToString } from 'react-dom/server'
import { z } from 'zod'

type Env = {
  NODE_ENV?: string
  SERVER_PORT?: string
}

type HonoEnv = {
  Bindings: Env
}

const MessageSchema = z.union([
  z.object({
    role: z.enum(['system']),
    content: z.string().min(1),
  }),
  z.object({
    role: z.enum(['assistant']),
    content: z.string().min(1),
  }),
  z.object({
    role: z.enum(['user']),
    content: z.union([
      z.string().min(1),
      z
        .union([
          z.object({
            type: z.enum(['text']),
            text: z.string().min(1),
          }),
          z.object({
            type: z.enum(['image_url']),
            image_url: z.object({
              url: z.string().min(1),
              detail: z.enum(['auto', 'low', 'high']).optional(),
            }),
          }),
        ])
        .array(),
    ]),
  }),
])

const app = new Hono<HonoEnv>()
  .post(
    '/api/profile',
    sValidator(
      'form',
      z.object({
        name: z.string().min(2, { message: 'nameã¯2æ–‡å­—ä»¥ä¸Šã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“' }),
        email: z.string().email({ message: 'emailã¯æ­£ã—ã„å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“' }),
      }),
      (values, c) => {
        const { success, error = [] } = values as {
          success: boolean
          error?: { message: string }[]
        }
        if (success) {
          return
        }
        return c.json({ error: error.map((x) => x.message).join(',') || '' }, 400)
      },
    ),
    (c) => {
      const { name, email } = c.req.valid('form') // form-data; ã§å—ã‘å–ã‚‹å ´åˆ
      console.log('req', { name, email })
      return c.json({ name, email, updated_at: new Date().toISOString() })
    },
  )
  .post(
    '/api/chat',
    validator('header', (value, c) => {
      const apiKey = value['api-key']
      const baseURL = value['base-url']
      const fakeMode = apiKey === 'fakemode' && apiKey === 'fakemode'
      const mcpServerURLs = value['mcp-server-urls']
      if (!apiKey) {
        return c.json({ message: `Validation Error: Missing required header 'api-key'` }, 400)
      }
      if (!baseURL) {
        return c.json({ message: `Validation Error: Missing required header 'base-url'` }, 400)
      }
      if (!fakeMode && !z.string().url().safeParse(baseURL).success) {
        return c.json({ message: `Validation Error: Invalid url 'base-url'` }, 400)
      }
      const { SERVER_PORT: port } = env<Env>(c)
      const fakeBaseURL = `http://localhost:${port || 3000}/api`
      return {
        'api-key': apiKey,
        'base-url': fakeMode ? fakeBaseURL : baseURL,
        'mcp-server-urls': mcpServerURLs,
      }
    }),
    sValidator(
      'json',
      z.object({
        messages: MessageSchema.array(),
        model: z.string().min(1),
        stream: z.boolean().default(false),
        temperature: z.number().min(0).max(1).optional(),
        max_tokens: z.number().min(1).optional(),
        stream_options: z
          .object({
            include_usage: z.boolean().optional(),
          })
          .optional(),
      }),
    ),
    async (c) => {
      const header = c.req.valid('header')
      const req = c.req.valid('json')
      try {
        const openai = new OpenAI({
          apiKey: header['api-key'],
          baseURL: header['base-url'],
          fetch: async (url, options = {}) => {
            // ã‚«ã‚¹ã‚¿ãƒ ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
            const customHeaders = {
              'mcp-server-urls': header['mcp-server-urls'],
            }
            // options.headers ã®å‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€é©åˆ‡ã«ãƒãƒ¼ã‚¸
            let existingHeaders: Record<string, string> = {}

            if (options.headers instanceof Headers) {
              // Headersã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã¯å®‰å…¨ã«Record<string, string>ã«å¤‰æ›
              existingHeaders = {}
              options.headers.forEach((value, key) => {
                if (typeof key === 'string' && typeof value === 'string') {
                  existingHeaders[key] = value
                }
              })
            } else if (typeof options.headers === 'object' && options.headers !== null) {
              // plain objectã®å ´åˆã¯å®‰å…¨ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã‚³ãƒ”ãƒ¼
              existingHeaders = Object.fromEntries(
                Object.entries(options.headers).filter(
                  ([key, value]) => typeof key === 'string' && typeof value === 'string',
                ),
              )
            }

            // ãƒãƒ¼ã‚¸ã—ã¦æ–°ã—ã„ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚»ãƒƒãƒˆ
            options.headers = {
              ...existingHeaders,
              ...customHeaders,
            }
            return fetch(url, options)
          },
        })
        const completion = await openai.chat.completions.create({
          messages: req.messages,
          model: req.model,
          stream: req.stream,
          temperature: req.temperature,
          max_tokens: req.max_tokens,
          stream_options: req.stream_options,
        })
        return req.stream
          ? streamSSE(c, async (stream) => {
              let aborted = false
              stream.onAbort(() => {
                aborted = true
                completionStream.controller.abort()
              })
              const completionStream = completion as Stream<OpenAI.ChatCompletionChunk>
              for await (const chunk of completionStream) {
                await stream.writeSSE({ data: JSON.stringify(chunk) })
                await new Promise((resolve) => setTimeout(resolve, 10))
              }
              if (!aborted) {
                await stream.writeSSE({ data: '[DONE]' })
              }
            })
          : c.json(completion as OpenAI.ChatCompletion)
      } catch (e) {
        console.error(e)
        return c.json({ message: e instanceof Error && e.message }, 500)
      }
    },
  )
  .post(
    '/api/chat/completions',
    sValidator(
      'json',
      z.object({
        messages: MessageSchema.array(),
        model: z.string().min(1),
        stream: z.boolean().default(false),
        temperature: z.number().min(0).max(1).optional(),
        max_tokens: z.number().min(1).optional(),
        stream_options: z
          .object({
            include_usage: z.boolean().optional(),
          })
          .optional(),
      }),
    ),
    async (c) => {
      const req = c.req.valid('json')

      console.log('[Request]-->')
      console.dir(req, { depth: null })
      console.log('<--[Request]')

      const splitByLength = (text: string, length: number): string[] => {
        const result = []
        for (let i = 0; i < text.length; i += length) {
          result.push(text.slice(i, i + length))
        }
        return result
      }

      const filePath = path.join(process.cwd(), 'src/server/data/test.md')
      const content = fs.readFileSync(filePath, 'utf8')
      await new Promise((resolve) => setTimeout(resolve, 3000)) // delay
      return req.stream
        ? streamSSE(c, async (stream) => {
            let aborted = false
            stream.onAbort(() => {
              aborted = true
            })
            const chunkResponse: OpenAI.ChatCompletionChunk = {
              id: 'chatcmpl-1234567890abcdef',
              object: 'chat.completion.chunk',
              created: Math.floor(Date.now() / 1000),
              model: req.model,
              choices: [
                {
                  index: 0,
                  delta: {
                    role: 'assistant',
                    content: '',
                  },
                  finish_reason: null,
                },
              ],
              usage: null,
            }
            const chunkSize = 5
            const repeatCount = 3
            const contentList = [
              `ã“ã‚Œã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§è¿”ã™ãƒ‡ãƒ¼ã‚¿ã¯ ${repeatCount} å›ç¹°ã‚Šè¿”ã—ã¾ã™ ğŸš€`,
              '\n\n',
              ...splitByLength(content.repeat(repeatCount), chunkSize),
              'ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®çµ‚ç«¯ã§ã™ ğŸš€',
            ]
            const reasoningContent =
              'ã“ã¡ã‚‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ã€æ¨è«–ã‚’è¡Œã†ãŸã‚ã®åŸºç›¤ã¨ãªã‚‹æƒ…å ±ã‚„çŸ¥è­˜ã‚’æä¾›ã—ã€ã•ã¾ã–ã¾ãªçŠ¶æ³ã‚„å•é¡Œã«å¯¾ã—ã¦è«–ç†çš„ã«è€ƒãˆã‚‹åŠ›ã‚’é¤Šã†ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚\nã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šæ·±ã„ç†è§£ã¨æ­£ç¢ºãªçµè«–ã‚’å°ãå‡ºã™ãŸã‚ã®æ€è€ƒã®æ çµ„ã¿ã‚’æ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã“ã¨ã‚’æ„å›³ã—ã¦ã„ã¾ã™ã€‚\nãã—ã¦ã“ã‚Œã¯ãƒ€ãƒŸãƒ¼ã®æ¨è«–ã§ã™ã€‚'
            const chunkList = [
              ...splitByLength(reasoningContent, chunkSize).map((text) => ({
                content: undefined,
                reasoning_content: text,
              })),
              ...contentList.map((text) => ({
                content: text,
                reasoning_content: undefined,
              })),
            ]
            for (const chunk of chunkList) {
              if (aborted) {
                break
              }
              await stream.writeSSE({
                data: JSON.stringify({
                  ...chunkResponse,
                  choices: [
                    {
                      ...chunkResponse.choices[0],
                      delta: {
                        role: 'assistant',
                        content: chunk.content,
                        reasoning_content: chunk.reasoning_content,
                      },
                    },
                  ],
                }),
              })
              await stream.sleep(35) // delay
            }
            if (aborted) {
              return
            }
            if (req.stream_options?.include_usage) {
              await stream.writeSSE({
                data: JSON.stringify({
                  ...chunkResponse,
                  choices: [
                    {
                      ...chunkResponse.choices[0],
                      delta: null,
                    },
                  ],
                  usage: {
                    prompt_tokens: 10,
                    completion_tokens: 20,
                    total_tokens: 30,
                  },
                }),
              })
            }
            await stream.writeSSE({ data: '[DONE]' })
          })
        : c.json({
            id: 'chatcmpl-1234567890abcdef',
            object: 'chat.completion',
            created: Math.floor(Date.now() / 1000),
            model: req.model,
            choices: [
              {
                index: 0,
                message: {
                  role: 'assistant',
                  content,
                },
                finish_reason: 'stop',
              },
            ],
            usage: {
              prompt_tokens: 10,
              completion_tokens: 20,
              total_tokens: 30,
            },
          } as OpenAI.ChatCompletion)
    },
  )
  .get('*', (c) => {
    const { NODE_ENV } = env<Env>(c)
    const prod = NODE_ENV === 'production'
    return c.html(
      renderToString(
        <html lang='ja'>
          <head>
            <meta charSet='utf-8' />
            <meta content='width=device-width, initial-scale=1' name='viewport' />
            <title>Portfolio</title>
            <link rel='icon' href={prod ? '/static/favicon.ico' : 'favicon.ico'} />
            <link rel='stylesheet' href={prod ? '/static/main.css' : '/src/client/main.css'} />
            <script type='module' src={prod ? '/static/client.js' : '/src/client/main.tsx'} />
          </head>
          <body>
            <div id='root' />
          </body>
        </html>,
      ),
    )
  })
  .onError((err, c) => {
    return c.json({ error: err.message }, 500)
  })

export type AppType = typeof app
export default app
