import { ReadableStream } from 'node:stream/web'
import type { LLMProvider, Messages, Reader } from './types'

const TEST_DATA = `
ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§ã™ğŸ¤–

ä½•å›ã‹ç¹°ã‚Šè¿”ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡ºåŠ›ã—ã¾ã™ã€‚

---

# è¦‹å‡ºã—1
## è¦‹å‡ºã—2
### è¦‹å‡ºã—3
#### è¦‹å‡ºã—4
##### è¦‹å‡ºã—5

ã“ã‚Œã¯é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚

**å¤ªå­—ãƒ†ã‚­ã‚¹ãƒˆ**

*æ–œä½“ãƒ†ã‚­ã‚¹ãƒˆ*

~~å–ã‚Šæ¶ˆã—ç·šãƒ†ã‚­ã‚¹ãƒˆ~~

- ãƒªã‚¹ãƒˆé …ç›®1
- ãƒªã‚¹ãƒˆé …ç›®2
    - ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒªã‚¹ãƒˆé …ç›®1
    - ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒªã‚¹ãƒˆé …ç›®2
- ãƒªã‚¹ãƒˆé …ç›®3

1. ç•ªå·ä»˜ããƒªã‚¹ãƒˆé …ç›®1
2. ç•ªå·ä»˜ããƒªã‚¹ãƒˆé …ç›®2
    1. ãƒã‚¹ãƒˆã•ã‚ŒãŸç•ªå·ä»˜ããƒªã‚¹ãƒˆé …ç›®1
    2. ãƒã‚¹ãƒˆã•ã‚ŒãŸç•ªå·ä»˜ããƒªã‚¹ãƒˆé …ç›®2
3. ç•ªå·ä»˜ããƒªã‚¹ãƒˆé …ç›®3

> ã“ã‚Œã¯å¼•ç”¨ã§ã™ã€‚
> è¤‡æ•°è¡Œã«æ¸¡ã‚‹å¼•ç”¨ã®ä¾‹ã§ã™ã€‚

\`ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰\`

\`\`\`
ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã§ã™ã€‚
è¤‡æ•°è¡Œã«ã‚ãŸã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
\`\`\`

[ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆ](https://example.com)

ç”»åƒã§ã™ã€‚

![ç”»åƒã®ä»£æ›¿ãƒ†ã‚­ã‚¹ãƒˆ](https://picsum.photos/200)

---

ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¾‹ã§ã™ã€‚

| å•†å“å  | ä¾¡æ ¼    |
|---------|---------|
| ã‚Šã‚“ã”  | Â¥100    |
| ã¿ã‹ã‚“  | Â¥80     |
| ãƒãƒŠãƒŠ  | Â¥120    |

Pythonã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã§ã™ã€‚

\`\`\`python
def hello():
    output = "Hello, World!"
    print(output)

if __name__ == "__main__":
    hello()
\`\`\`

TypeScriptã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã§ã™ã€‚

\`\`\`ts
interface Person {
  name: string;
  age: number;
  isStudent: boolean;
}
\`\`\`
`
const CHUNK_SIZE = 5
const MAX_CHUNKS = (TEST_DATA.length / CHUNK_SIZE) * 3

function createDummyStream(messages: Messages[]): ReadableStreamDefaultReader<Uint8Array> {
  let index = 0
  let chunkCount = 0

  const createResponsePayload = ({
    id,
    model = 'dummy',
    content,
    finish_reason = null,
    usage = null,
  }: {
    id: string
    model?: string
    content: string
    finish_reason?: 'length' | 'stop' | null
    usage?: {
      prompt_tokens: number
      completion_tokens: number
      total_tokens: number
    } | null
  }): string => {
    const data = {
      id,
      model,
      choices: [{ delta: { content }, finish_reason }],
      usage,
    }
    return `data: ${JSON.stringify(data)}\n`
  }

  const stream = new ReadableStream<Uint8Array>({
    async pull(controller) {
      let chunk = ''
      for (let i = 0; i < CHUNK_SIZE; i++) {
        chunk += TEST_DATA[index]
        index = (index + 1) % TEST_DATA.length
      }

      controller.enqueue(
        new TextEncoder().encode(
          createResponsePayload({
            id: `${chunkCount}`,
            content: chunk,
          }),
        ),
      )
      chunkCount++

      await new Promise((resolve) => setTimeout(resolve, 25)) // delay

      if (chunkCount >= MAX_CHUNKS) {
        const sloppyInputToken = messages.map((x) => x.content).join('').length // é©å½“ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°
        controller.enqueue(
          new TextEncoder().encode(
            createResponsePayload({
              id: `${chunkCount}`,
              content: 'Test stream end ğŸš€',
              finish_reason: 'stop',
              usage: {
                // ãƒ†ã‚¹ãƒˆç”¨ã«é©å½“ã«å€¤ã‚’åŸ‹ã‚ã‚‹
                prompt_tokens: sloppyInputToken,
                completion_tokens: chunkCount,
                total_tokens: sloppyInputToken + chunkCount,
              },
            }),
          ),
        )
        controller.enqueue(new TextEncoder().encode('data: [DONE]\n'))
        controller.close()
      }
    },
  })

  return stream.getReader()
}

export class TestProvider implements LLMProvider {
  async chatStream(
    messages: Messages[],
    _temperature?: number | null,
    _maxTokens?: number | null,
  ): Promise<Reader> {
    await new Promise((resolve) => setTimeout(resolve, 1000)) // delay
    return createDummyStream(messages)
  }
}
